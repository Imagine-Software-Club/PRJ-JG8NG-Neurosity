{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Method: Initial Point [4 4]\n",
      "    Iterated 20 times\n",
      "    Minimizer found at approximately [1.0000018496881817,1.0000018496881817][1.0000018496881817,1.0000018496881817]\n",
      "    Function value: 3.5000000000068425\n",
      "\n",
      "Gradient Method: Initial Point [0.25 0.25]\n",
      "    Iterated 5 times\n",
      "    Minimizer found at approximately [0.9999973323820796,0.9999973323820796][0.9999973323820796,0.9999973323820796]\n",
      "    Function value: 3.500000000014232\n",
      "\n",
      "Gradient Method: Initial Point [50 10]\n",
      "    Iterated 27 times\n",
      "    Minimizer found at approximately [0.999999999979472,0.999999999979472][0.999999999979472,0.999999999979472]\n",
      "    Function value: 3.5\n",
      "\n",
      "Gradient Method: Initial Point [30 30]\n",
      "    Iterated 25 times\n",
      "    Minimizer found at approximately [0.999999999979472,0.999999999979472][0.999999999979472,0.999999999979472]\n",
      "    Function value: 3.5\n",
      "\n",
      "Damped Newton Method Initial Point [4 4]\n",
      "    Iterated 7 times\n",
      "    Minimizer found at approximately [1.0000002973152278,1.0000002973152278][1.0000002973152278,1.0000002973152278]\n",
      "    Function value: 3.5000000000001767\n",
      "\n",
      "Damped Newton Method Initial Point [0.25 0.25]\n",
      "    Iterated 3 times\n",
      "    Minimizer found at approximately [-2.9864466748099477e-12,-2.9864466748099477e-12][-2.9864466748099477e-12,-2.9864466748099477e-12]\n",
      "    Function value: 4.0\n",
      "\n",
      "Damped Newton Method Initial Point [50 10]\n",
      "    Iterated 13 times\n",
      "    Minimizer found at approximately [1.0000001231998767,1.0000001231998767][1.0000000665612037,1.0000000665612037]\n",
      "    Function value: 3.5000000000000213\n",
      "\n",
      "Damped Newton Method Initial Point [30 30]\n",
      "    Iterated 12 times\n",
      "    Minimizer found at approximately [1.000000331771727,1.000000331771727][1.000000331771727,1.000000331771727]\n",
      "    Function value: 3.5000000000002203\n",
      "\n",
      "Hybrid method Initial Point [4 4]\n",
      "    Iterated 7 times\n",
      "    Minimizer found at approximately [1.0000002973152278,1.0000002973152278][1.0000002973152278,1.0000002973152278]\n",
      "    Function value: 3.5000000000001767\n",
      "\n",
      "Hybrid method Initial Point [0.25 0.25]\n",
      "    Iterated 17 times\n",
      "    Minimizer found at approximately [0.9999974575050667,0.9999974575050667][0.9999974575050667,0.9999974575050667]\n",
      "    Function value: 3.5000000000129283\n",
      "\n",
      "Hybrid method Initial Point [50 10]\n",
      "    Iterated 13 times\n",
      "    Minimizer found at approximately [1.0000001231998767,1.0000001231998767][1.0000000665612037,1.0000000665612037]\n",
      "    Function value: 3.5000000000000213\n",
      "\n",
      "Hybrid method Initial Point [30 30]\n",
      "    Iterated 12 times\n",
      "    Minimizer found at approximately [1.000000331771727,1.000000331771727][1.000000331771727,1.000000331771727]\n",
      "    Function value: 3.5000000000002203\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def gradient_method_with_print(x0, f, grad, alpha=0.5, beta=0.5, s=1, tol=1e-5):\n",
    "    x = x0\n",
    "    iters = 0\n",
    "    print(f\"Initial point: {x}\")\n",
    "    while np.linalg.norm(grad(x)) > tol:\n",
    "        dx = -grad(x)\n",
    "        step = backtrackingLineSearch(x, dx, f, grad, alpha, beta, s)\n",
    "        x = x + step * dx\n",
    "        iters += 1\n",
    "        print(f\"Iteration {iters}: Minimizer: {x}, Function Value: {f(x)}\")\n",
    "    print(f\"Total Iterations: {iters}\")\n",
    "    return x, iters\n",
    "def functionValue(x):\n",
    "    return 0.25 * x[0]**4 + 0.25 * x[1]**4 - x[0]*x[1] + 4\n",
    "\n",
    "def gradient(x):\n",
    "    return np.array([x[0]**3 - x[1], x[1]**3 - x[0]])\n",
    "\n",
    "def backtrackingLineSearch(x, dx, f, grad, alpha=0.5, beta=0.5, s=1):\n",
    "    while f(x + s*dx) > f(x) + alpha * s * np.dot(grad(x).T, dx):\n",
    "        s = beta * s\n",
    "    return s\n",
    "\n",
    "initialPoints = [np.array([4, 4]), np.array([0.25, 0.25]), np.array([50, 10]), np.array([30, 30])]\n",
    "\n",
    "def gradient_method_output(x0, f, grad, alpha=0.5, beta=0.5, s=1, tol=1e-5):\n",
    "    x = x0\n",
    "    iters = 0\n",
    "    while np.linalg.norm(grad(x)) > tol:\n",
    "        dx = -grad(x)\n",
    "        step = backtrackingLineSearch(x, dx, f, grad, alpha, beta, s)\n",
    "        x = x + step * dx\n",
    "        iters += 1\n",
    "    return f\"Gradient Method: Initial Point {x0}\\n    Iterated {iters} times\\n    Minimizer found at approximately [{x[0]},{x[0]}][{x[1]},{x[1]}]\\n    Function value: {f(x)}\"\n",
    "\n",
    "def damped_newtons_method_output(x0, f, grad, hess, alpha=0.5, beta=0.5, s=1, tol=1e-5):\n",
    "    x = x0\n",
    "    iters = 0\n",
    "    while np.linalg.norm(grad(x)) > tol:\n",
    "        dx = -np.linalg.inv(hess(x)).dot(grad(x))\n",
    "        step = backtrackingLineSearch(x, dx, f, grad, alpha, beta, s)\n",
    "        x = x + step * dx\n",
    "        iters += 1\n",
    "    return f\"Damped Newton Method Initial Point {x0}\\n    Iterated {iters} times\\n    Minimizer found at approximately [{x[0]},{x[0]}][{x[1]},{x[1]}]\\n    Function value: {f(x)}\"\n",
    "\n",
    "def hybrid_newtons_method_output(x0, f, grad, hess, alpha=0.5, beta=0.5, s=1, tol=1e-5):\n",
    "    x = x0\n",
    "    iters = 0\n",
    "    while np.linalg.norm(grad(x)) > tol:\n",
    "        eigenValues = np.linalg.eigvals(hess(x))\n",
    "        if np.all(eigenValues > 0):\n",
    "            dx = -np.linalg.inv(hess(x)).dot(grad(x))\n",
    "        else:\n",
    "            dx = -grad(x)\n",
    "        step = backtrackingLineSearch(x, dx, f, grad, alpha, beta, s)\n",
    "        x = x + step * dx\n",
    "        iters += 1\n",
    "    return f\"Hybrid method Initial Point {x0}\\n    Iterated {iters} times\\n    Minimizer found at approximately [{x[0]},{x[0]}][{x[1]},{x[1]}]\\n    Function value: {f(x)}\"\n",
    "\n",
    "output = []\n",
    "for x0 in initialPoints:\n",
    "    output.append(gradient_method_output(x0, functionValue, gradient))\n",
    "for x1 in initialPoints:\n",
    "    output.append(damped_newtons_method_output(x1, functionValue, gradient, hessian))\n",
    "for x2 in initialPoints: \n",
    "    output.append(hybrid_newtons_method_output(x2, functionValue, gradient, hessian))\n",
    "\n",
    "print(\"\\n\\n\".join(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
